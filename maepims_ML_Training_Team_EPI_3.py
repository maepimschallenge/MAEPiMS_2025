# -*- coding: utf-8 -*-
"""MAEPiMS_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ca0pj1g1WEzScqDsKciP1M2z8MS0dfNr
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the PINN model
class SEIR_PINN(nn.Module):
    def __init__(self):
        super(SEIR_PINN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 40), nn.Tanh(),
            nn.Linear(40, 40), nn.Tanh(),
            nn.Linear(40, 40), nn.Tanh(),
            nn.Linear(40, 6)  # Output (S, E, T, I, R, D)
        )

    def forward(self, t, j):
        inputs = torch.cat([t, j], dim=1)  # Concatenate time and zone index
        return self.net(inputs)

def loss_function(model, t_train, j_train, S_train, E_train, T_train, I_train, R_train, D_train, params, λ_physics=10.0, λ_data=1.0):
    # Convert to PyTorch tensors
    t_train_tensor = torch.tensor(t_train, dtype=torch.float32, requires_grad=True).view(-1, 1)
    j_train_tensor = torch.tensor(j_train, dtype=torch.float32).view(-1, 1)

    # Compute PINN predictions
    S_pred, E_pred, T_pred, I_pred, R_pred, D_pred = model(t_train_tensor, j_train_tensor).T

    # Compute derivatives using autograd
    S_t = torch.autograd.grad(S_pred, t_train_tensor, grad_outputs=torch.ones_like(S_pred), create_graph=True)[0]
    E_t = torch.autograd.grad(E_pred, t_train_tensor, grad_outputs=torch.ones_like(E_pred), create_graph=True)[0]
    T_t = torch.autograd.grad(T_pred, t_train_tensor, grad_outputs=torch.ones_like(T_pred), create_graph=True)[0]
    I_t = torch.autograd.grad(I_pred, t_train_tensor, grad_outputs=torch.ones_like(I_pred), create_graph=True)[0]
    R_t = torch.autograd.grad(R_pred, t_train_tensor, grad_outputs=torch.ones_like(R_pred), create_graph=True)[0]
    D_t = torch.autograd.grad(D_pred, t_train_tensor, grad_outputs=torch.ones_like(D_pred), create_graph=True)[0]

    # Extract parameters
    β_max, ρ, f, θ, ω, α, σ, γ, δ = params

    # Define time-dependent infection rate β_j(t)
    β_j_t = β_max / (1 + torch.exp(-(1 - ρ) * (t_train_tensor - t_train_tensor.min())))

    # Physics loss enforcing the equations
    loss_S = S_t + β_j_t * S_pred * I_pred * (1 - f) - θ * T_pred - ω * R_pred
    loss_E = E_t - β_j_t * S_pred * I_pred * (1 - f) + α * E_pred
    loss_T = T_t - α * E_pred + (σ + θ) * T_pred
    loss_I = I_t - σ * T_pred + (γ + δ) * I_pred
    loss_R = R_t - γ * I_pred + ω * R_pred
    loss_D = D_t - δ * I_pred

    # Data loss (forcing model to fit real-world data)
    data_loss = torch.mean((I_pred - torch.tensor(I_train, dtype=torch.float32)) ** 2)

    # Weighted physics loss
    physics_loss = torch.mean(loss_S ** 2 + loss_E ** 2 + loss_T ** 2 + loss_I ** 2 + loss_R ** 2 + loss_D ** 2)

    # Total loss with weights
    total_loss = λ_physics * physics_loss + λ_data * data_loss

    return total_loss

import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler

# Load COVID-19 data
file_path = "/content/comprehensive_covid_data_nigeria.csv"
data = pd.read_csv(file_path)

# Convert date column to datetime and create day index
data['Date'] = pd.to_datetime(data['Date'])
data['day'] = (data['Date'] - data['Date'].min()).dt.days

# Drop missing values
data = data.dropna()

# Define mapping from region names to numerical indices
zone_mapping = {
    'North Central': 1,
    'North East': 2,
    'North West': 3,
    'South East': 4,
    'South South': 5,
    'South West': 6
}

# Convert region names to numeric indices
data['Zone'] = data['Zone'].map(zone_mapping)

# Drop rows with NaN zones (if any unexpected values exist)
data = data.dropna(subset=['Zone'])

# Convert zone to integer
data['Zone'] = data['Zone'].astype(int)

# Extract relevant columns
t_train = data['day'].values.reshape(-1, 1)  # Time in days
j_train = data['Zone'].values.reshape(-1, 1)  # Zone index (1-6)
I_train = data['Cases'].values.reshape(-1, 1)  # Confirmed cases

# Define initial populations for each zone
N_j = np.array([29_252_408, 26_263_866, 48_942_307, 21_955_414, 28_829_288, 46_706_662], dtype=np.float32)

# Compute initial states
S_train = np.array([N_j[j - 1] - I_train[i] for i, j in enumerate(j_train)], dtype=np.float32)
E_train = (I_train * 0.1).astype(np.float32)
T_train = (I_train * 0.05).astype(np.float32)
R_train = (I_train * 0.03).astype(np.float32)
D_train = (I_train * 0.02).astype(np.float32)

# Initialize Min-Max Scalers
scaler_t = MinMaxScaler()
scaler_I = MinMaxScaler()
scaler_S = MinMaxScaler()
scaler_E = MinMaxScaler()
scaler_T = MinMaxScaler()
scaler_R = MinMaxScaler()
scaler_D = MinMaxScaler()

# Normalize the data
t_train_scaled = scaler_t.fit_transform(t_train)
I_train_scaled = scaler_I.fit_transform(I_train)
S_train_scaled = scaler_S.fit_transform(S_train)
E_train_scaled = scaler_E.fit_transform(E_train)
T_train_scaled = scaler_T.fit_transform(T_train)
R_train_scaled = scaler_R.fit_transform(R_train)
D_train_scaled = scaler_D.fit_transform(D_train)

# Convert to PyTorch tensors
t_train_tensor = torch.tensor(t_train_scaled, dtype=torch.float32)
j_train_tensor = torch.tensor(j_train, dtype=torch.long)  # Zone index as long tensor
S_train_tensor = torch.tensor(S_train_scaled, dtype=torch.float32)
E_train_tensor = torch.tensor(E_train_scaled, dtype=torch.float32)
T_train_tensor = torch.tensor(T_train_scaled, dtype=torch.float32)
I_train_tensor = torch.tensor(I_train_scaled, dtype=torch.float32)
R_train_tensor = torch.tensor(R_train_scaled, dtype=torch.float32)
D_train_tensor = torch.tensor(D_train_scaled, dtype=torch.float32)

# Function to denormalize predictions
def denormalize_predictions(S_pred, E_pred, T_pred, I_pred, R_pred, D_pred):
    S_pred = scaler_S.inverse_transform(S_pred)
    E_pred = scaler_E.inverse_transform(E_pred)
    T_pred = scaler_T.inverse_transform(T_pred)
    I_pred = scaler_I.inverse_transform(I_pred)
    R_pred = scaler_R.inverse_transform(R_pred)
    D_pred = scaler_D.inverse_transform(D_pred)
    return S_pred, E_pred, T_pred, I_pred, R_pred, D_pred

# Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move model to device
model = SEIR_PINN().to(device)

# Convert training data to PyTorch tensors and move to device
t_train_tensor = torch.tensor(t_train, dtype=torch.float32, device=device).view(-1, 1)
t_train_tensor.requires_grad = True  # Needed for autograd

j_train_tensor = torch.tensor(j_train, dtype=torch.float32, device=device).view(-1, 1)
S_train_tensor = torch.tensor(S_train, dtype=torch.float32, device=device).view(-1, 1)
E_train_tensor = torch.tensor(E_train, dtype=torch.float32, device=device).view(-1, 1)
T_train_tensor = torch.tensor(T_train, dtype=torch.float32, device=device).view(-1, 1)
I_train_tensor = torch.tensor(I_train, dtype=torch.float32, device=device).view(-1, 1)
R_train_tensor = torch.tensor(R_train, dtype=torch.float32, device=device).view(-1, 1)
D_train_tensor = torch.tensor(D_train, dtype=torch.float32, device=device).view(-1, 1)

# epidemiological parameters
β_max = 0.5
ρ = 0.3  # Lockdown
f = 0.8  # Face mask
θ = 0.1 #Rate of tested negative
ω = 0.05 #Resusceptibility
α = 0.1 #Testing
σ = 0.2 #confirmed cases rate
γ = 0.15 #Cured rate
δ = 0.01 #Death rate

params = (β_max, ρ, f, θ, ω, α, σ, γ, δ)

# optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# setting training loop
epochs = 500
for epoch in range(epochs):
    optimizer.zero_grad()

    # Compute loss
    loss = loss_function(model, t_train_tensor, j_train_tensor,
                         S_train_tensor, E_train_tensor, T_train_tensor,
                         I_train_tensor, R_train_tensor, D_train_tensor, params)

    #perform Backpropagation
    loss.backward()
    optimizer.step()

    # Print out after every 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

# Saving the trained model
torch.save(model.state_dict(), "pinn_seir_nigeria_zones.pth")

import numpy as np
import torch
import matplotlib.pyplot as plt

# Ensure t_train is a proper 1D NumPy array
t_train = np.array(t_train).flatten()

# Set model to evaluation mode
model.eval()

# Generate test data for 2022
t_test = np.arange(t_train.max() + 1, t_train.max() + 365 + 1)  # Predict for a full year

# Normalize test time data
t_test_scaled = scaler_t.transform(t_test.reshape(-1, 1))  # Reshape before normalization

# Repeat each day for all 6 zones
t_test_scaled = np.repeat(t_test_scaled, 6, axis=0)  # Now 2190 rows to match j_test
j_test = np.tile(np.arange(1, 7), len(t_test))  # Ensure each day has all zones

# Convert to PyTorch tensors
t_test_tensor = torch.tensor(t_test_scaled, dtype=torch.float32).view(-1, 1)
j_test_tensor = torch.tensor(j_test, dtype=torch.long).view(-1, 1)

# Make predictions
with torch.no_grad():
    preds = model(t_test_tensor, j_test_tensor)

# Extract predictions & reshape them into 2D format
S_pred, E_pred, T_pred, I_pred, R_pred, D_pred = preds.numpy().T

# Reshape into 2D for denormalization
S_pred = S_pred.reshape(-1, 1)
E_pred = E_pred.reshape(-1, 1)
T_pred = T_pred.reshape(-1, 1)
I_pred = I_pred.reshape(-1, 1)
R_pred = R_pred.reshape(-1, 1)
D_pred = D_pred.reshape(-1, 1)

# Denormalize predictions
S_pred, E_pred, T_pred, I_pred, R_pred, D_pred = denormalize_predictions(S_pred, E_pred, T_pred, I_pred, R_pred, D_pred)

# Plot predictions
plt.figure(figsize=(12, 6))
plt.plot(t_train, I_train, '--', label="Actual Cases (2020-2021)")

# Average over zones for plotting clarity
t_test_days = np.arange(t_train.max() + 1, t_train.max() + 365 + 1)
I_pred_avg = I_pred.reshape(-1, 6).mean(axis=1)  # Average across zones

plt.plot(t_test_days, I_pred_avg, 'b-', label="Predicted Cases (2022)")
plt.xlabel("Days since first case")
plt.ylabel("Daily Cases")
plt.legend()
plt.title("COVID-19 Forecast for Nigeria's Geopolitical Zones (2022)")
plt.show()

print(I_pred_avg[:10])  # Print first 10 predicted values